{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f7d32a",
   "metadata": {},
   "source": [
    "\n",
    "Part B\n",
    "\n",
    "Pick your favorite between 'query_original' and 'query_dt_c' and let's add some more functionality.\n",
    "\n",
    "Queries used daily are stored in a csv file in the drive(queries.csv).\n",
    "\n",
    "Make your class able to instantiate from this csv and create as many instances as lines existing in the csv.\n",
    "Each line in the csv has two columns 'Query' and 'Number' as supposed.\n",
    "Csv file path argument shall be optional. If a path is provided it will be used or a default path should be used.\n",
    "Print the 'query' string of all instances created from the csv. (not instances as whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, query_string: str, no_of_query: int = 0):\n",
    "        self.query_string = query_string\n",
    "        self.no_of_query = no_of_query\n",
    "\n",
    "    @property\n",
    "    def no_of_query(self):\n",
    "        return self._no_of_query\n",
    "\n",
    "    @no_of_query.setter\n",
    "    def no_of_query(self, n):\n",
    "        if n < 0:\n",
    "            raise ValueError(\n",
    "                f\"no_of_query must be zero or greater not {n}\"\n",
    "            )\n",
    "        self._no_of_query = n\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__} object qstring={self.query_string}, no_of_query={self.no_of_query}\"\n",
    "\n",
    "\n",
    "class QueryOriginal:\n",
    "    def __init__(self, queries: str | Iterable[Query] = \"queries.csv\"):\n",
    "        if isinstance(queries, str):\n",
    "            self._load_queries_from_csv(queries)\n",
    "        elif isinstance(queries, Iterable) and all(\n",
    "            map(lambda x: isinstance(x, Query), queries)\n",
    "        ):\n",
    "            self.query_list = [query for query in queries]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Can only pass a str or an Iterable of Query objects\"\n",
    "            )\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.query_list)\n",
    "\n",
    "    def _load_queries_from_csv(self, csvpath: str):\n",
    "        df = pd.read_csv(csvpath)\n",
    "        self.query_list = [Query(t[1], t[2]) for t in df.itertuples()]\n",
    "\n",
    "    def __str__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        queries = \"\\n\".join(map(str, self.query_list))\n",
    "        return f\"{class_name}: \\n{queries}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4bc01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_collection = QueryOriginal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0b17921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryOriginal: \n",
      "Query object qstring=csvtest1, no_of_query=11\n",
      "Query object qstring=csvtest2, no_of_query=22\n",
      "Query object qstring=csvtest3, no_of_query=33\n",
      "Query object qstring=csvtest0, no_of_query=0\n"
     ]
    }
   ],
   "source": [
    "print(query_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "076e4b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query object qstring=csvtest1, no_of_query=11\n",
      "Query object qstring=csvtest2, no_of_query=22\n",
      "Query object qstring=csvtest3, no_of_query=33\n",
      "Query object qstring=csvtest0, no_of_query=0\n"
     ]
    }
   ],
   "source": [
    "for query in query_collection:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987aa7f5",
   "metadata": {},
   "source": [
    "create a guessing game to be initialized as a class method. You can for example pick one random integer from 0-10 and have the user guess it in three tries.\n",
    "Hint: Method for step 3 will not access or use any class attribute. So what type of method will be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class GuessingGame:\n",
    "    @staticmethod\n",
    "    def start_game():\n",
    "        correct = random.randint(0, 10)\n",
    "        print(\"Start  of game, guess a number from 0 to 10\")\n",
    "        for i in range(3):\n",
    "            answer = None\n",
    "            while answer is None:\n",
    "                answer = input(f\"you got {3 - i} attempts left: \")\n",
    "                try:\n",
    "                    answer = int(answer)\n",
    "                except:\n",
    "                    answer = None\n",
    "            if answer == correct:\n",
    "                print(\"Congrats! You won!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"wrong answer\")\n",
    "        else:\n",
    "            print(\"Game Over\")\n",
    "        play_again = None\n",
    "        while play_again not in (\"y\", \"n\"):\n",
    "            play_again = input(\n",
    "                \"Would you like to play again? ('y', 'n'): \"\n",
    "            )\n",
    "        if play_again == \"y\":\n",
    "            GuessingGame.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b040ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessingGame.start_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e47585",
   "metadata": {},
   "source": [
    "Part C  \n",
    "\n",
    "Convert your web scraping code you developed previously into object oriented (use code from flexcar or atp rankings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898021",
   "metadata": {},
   "source": [
    "## Refactor ATP Scraping Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311416e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.FileHandler(\"log.txt\"))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Week:\n",
    "    def __init__(self, html_content: str):\n",
    "        self._soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        self._week = self._get_active_week()\n",
    "        self._ranks = list(range(1, 101))\n",
    "        self._player_names = self._get_player_names()\n",
    "        self._countries = self._get_countries()\n",
    "        self._points_all = self._get_points()\n",
    "        sizes = map(\n",
    "            len,\n",
    "            [\n",
    "                self._ranks,\n",
    "                self._player_names,\n",
    "                self._countries,\n",
    "                self._points_all,\n",
    "            ],\n",
    "        )\n",
    "        if not all(v > 0 for v in sizes):\n",
    "            logger.info(\n",
    "                f\"week {self._week} error\\nranks: {self._ranks}\\nplayer_names: {self._player_names}\\ncountries: {self._countries} \\n points_all: {self._points_all}\"\n",
    "            )\n",
    "\n",
    "    def to_list(self):\n",
    "        return [\n",
    "            {\n",
    "                \"week\": self._week,\n",
    "                \"rank\": rank,\n",
    "                \"player_name\": player_name,\n",
    "                \"country\": country,\n",
    "                \"points\": points,\n",
    "            }\n",
    "            for rank, player_name, country, points in zip(\n",
    "                self._ranks,\n",
    "                self._player_names,\n",
    "                self._countries,\n",
    "                self._points_all,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _get_active_week(self) -> str:\n",
    "        \"\"\"\n",
    "        returns the active week, directly through scraping and not through the url\n",
    "        unlike the get_all_urls it will extract the text contents instead of\n",
    "        the value attribute, otherwise we would get date 'Current-Date' for our\n",
    "        most recent date.\n",
    "        \"\"\"\n",
    "\n",
    "        select_tag = self._soup.find(\"select\", id=\"dateWeek-filter\")\n",
    "        return select_tag.find(\"option\", selected=True).text\n",
    "\n",
    "    def _get_player_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        returns a list of the top-100 players names ordered by rank\n",
    "        \"\"\"\n",
    "        li_tags = self._soup.find_all(\"li\", class_=\"name center\")\n",
    "        return [li.find(\"span\").text for li in li_tags]\n",
    "\n",
    "    def _get_countries(self):\n",
    "        \"\"\"\n",
    "        returns a list of the top-100 players countries ordered by rank\n",
    "        \"\"\"\n",
    "        svg_tags = self._soup.find_all(\"svg\", class_=\"atp-flag\")[:100]\n",
    "        use_tags = [svg_tag.find(\"use\") for svg_tag in svg_tags]\n",
    "        links = [use_tag[\"href\"] for use_tag in use_tags]\n",
    "        countries = [\n",
    "            self.__class__._extract_flag_abbr(link) for link in links\n",
    "        ]\n",
    "        return self._convert_flag_abbr(countries)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_flag_abbr(string: str) -> str:\n",
    "        \"\"\"\n",
    "        extract the flag abbreviation out of a link to the flag png\n",
    "        \"\"\"\n",
    "        return re.search(r\"(?<=#flag-)[A-Za-z]{3}$\", string).group(0)\n",
    "\n",
    "    def _convert_flag_abbr(self, countries: str) -> str:\n",
    "        \"\"\"\n",
    "        helper function to convert a flag/country abbreviation to the\n",
    "        actual country name. The relationship between the can be find inside\n",
    "        the source code and I did not have rely on external sources\n",
    "        \"\"\"\n",
    "        select_region_filter = self._soup.find(\n",
    "            \"select\", id=\"region-filter\"\n",
    "        )\n",
    "        region_option_tags = select_region_filter.find_all(\"option\")\n",
    "        countries_tuple = [\n",
    "            (region_option_tag[\"value\"], region_option_tag.text)\n",
    "            for region_option_tag in region_option_tags\n",
    "        ]\n",
    "        country_dict = {k.lower(): v for k, v in countries_tuple}\n",
    "        return [\n",
    "            country_dict.get(country_abbr, country_abbr)\n",
    "            for country_abbr in countries\n",
    "        ]\n",
    "\n",
    "    def _get_points(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        after some debugging a edge case failures, I adjusted and tested\n",
    "        the points extraction to the following code.\n",
    "        I essentially had to find the tag before the one I was looking for,\n",
    "        and the seek the sibling.\n",
    "        The reason for picking the slice is that I sometimes get 101 results\n",
    "        instead of 100 and in that case the first one does not lead to any points.\n",
    "        Taking the slice of the last 100 seems safe.\n",
    "        \"\"\"\n",
    "        points_tds = self._soup.find_all(\"td\", class_=\"age small-cell\")\n",
    "        return [\n",
    "            points_td.find_next_sibling(\"td\").find(\"a\").text.strip()\n",
    "            for points_td in points_tds[-100:]\n",
    "        ]\n",
    "\n",
    "\n",
    "class AtpDb:\n",
    "    def __init__(self):\n",
    "        self._headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
    "        }\n",
    "        url = \"https://www.atptour.com/en/rankings/singles\"\n",
    "        response = requests.get(url, headers=self._headers)\n",
    "        self._soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        self._response_contents = self._get_response_contents()\n",
    "        content_length = len(self._response_contents)\n",
    "        self._weeks = []\n",
    "        for i, html_content in enumerate(self._response_contents):\n",
    "            logger.info(\n",
    "                f\"parsing week {i + 1} out of {content_length}\"\n",
    "            )\n",
    "            self._weeks.extend(Week(html_content).to_list())\n",
    "\n",
    "    @property\n",
    "    def weeks(self):\n",
    "        return self._weeks\n",
    "\n",
    "    def _get_all_urls(self) -> list[str]:\n",
    "        \"\"\"Return a list with all weekly urls from the main soup object\"\"\"\n",
    "        select_tag = self._soup.find(\"select\", id=\"dateWeek-filter\")\n",
    "        date_tags = select_tag.find_all(\"option\")\n",
    "        dates = [\n",
    "            (\n",
    "                date_tag[\"value\"]\n",
    "                if not \"Current\" in date_tag[\"value\"]\n",
    "                else \"Current+Date\"\n",
    "            )\n",
    "            for date_tag in date_tags\n",
    "        ]\n",
    "        base_url = (\n",
    "            \"https://www.atptour.com/en/rankings/singles?dateWeek=\"\n",
    "        )\n",
    "        return sorted([f\"{base_url}{date}\" for date in dates])\n",
    "\n",
    "    def _get_response_contents(self):\n",
    "        urls = self._get_all_urls()\n",
    "        urls_length = len(urls)\n",
    "        response_content_list = []\n",
    "        for i, url in enumerate(urls):\n",
    "            logger.info(\n",
    "                f\"processing request {i + 1} out of {urls_length}\"\n",
    "            )\n",
    "            time.sleep(random.uniform(0.3, 0.4))\n",
    "            response = requests.get(url, headers=self._headers)\n",
    "            if response.status_code != 200:\n",
    "                response_content_list = []\n",
    "                raise ConnectionError(\n",
    "                    f\"failed to fetch data from {url}\"\n",
    "                )\n",
    "            response_content_list.append(response.content)\n",
    "        return response_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(AtpDb().weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4109eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bb_data_edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
