{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f7d32a",
   "metadata": {},
   "source": [
    "\n",
    "Part B\n",
    "\n",
    "Pick your favorite between 'query_original' and 'query_dt_c' and let's add some more functionality.\n",
    "\n",
    "Queries used daily are stored in a csv file in the drive(queries.csv).\n",
    "\n",
    "Make your class able to instantiate from this csv and create as many instances as lines existing in the csv.\n",
    "Each line in the csv has two columns 'Query' and 'Number' as supposed.\n",
    "Csv file path argument shall be optional. If a path is provided it will be used or a default path should be used.\n",
    "Print the 'query' string of all instances created from the csv. (not instances as whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11ac46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, query_string: str, no_of_query: int = 0):\n",
    "        self.query_string = query_string\n",
    "        self.no_of_query = no_of_query\n",
    "\n",
    "    @property\n",
    "    def no_of_query(self):\n",
    "        return self._no_of_query\n",
    "\n",
    "    @no_of_query.setter\n",
    "    def no_of_query(self, n):\n",
    "        if n < 0:\n",
    "            raise ValueError(\n",
    "                f\"no_of_query must be zero or greater not {n}\"\n",
    "            )\n",
    "        self._no_of_query = n\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__} object qstring={self.query_string}, no_of_query={self.no_of_query}\"\n",
    "\n",
    "\n",
    "class QueryOriginal:\n",
    "    def __init__(self, queries: str | Iterable[Query] = \"queries.csv\"):\n",
    "        if isinstance(queries, str):\n",
    "            self._load_queries_from_csv(queries)\n",
    "        elif isinstance(queries, Iterable) and all(\n",
    "            map(lambda x: isinstance(x, Query), queries)\n",
    "        ):\n",
    "            self.query_list = [query for query in queries]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Can only pass a str or an Iterable of Query objects\"\n",
    "            )\n",
    "    def __iter__(self):\n",
    "        return iter(self.query_list)\n",
    "\n",
    "    def _load_queries_from_csv(self, csvpath: str):\n",
    "        df = pd.read_csv(csvpath)\n",
    "        self.query_list = [Query(t[1], t[2]) for t in df.itertuples()]\n",
    "\n",
    "    def __str__(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        queries = \"\\n\".join(map(str, self.query_list))\n",
    "        return f\"{class_name}: \\n{queries}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4bc01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_collection = QueryOriginal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0b17921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryOriginal: \n",
      "Query object qstring=csvtest1, no_of_query=11\n",
      "Query object qstring=csvtest2, no_of_query=22\n",
      "Query object qstring=csvtest3, no_of_query=33\n",
      "Query object qstring=csvtest0, no_of_query=0\n"
     ]
    }
   ],
   "source": [
    "print(query_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "076e4b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query object qstring=csvtest1, no_of_query=11\n",
      "Query object qstring=csvtest2, no_of_query=22\n",
      "Query object qstring=csvtest3, no_of_query=33\n",
      "Query object qstring=csvtest0, no_of_query=0\n"
     ]
    }
   ],
   "source": [
    "for query in query_collection:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987aa7f5",
   "metadata": {},
   "source": [
    "create a guessing game to be initialized as a class method. You can for example pick one random integer from 0-10 and have the user guess it in three tries.\n",
    "Hint: Method for step 3 will not access or use any class attribute. So what type of method will be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc60f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class GuessingGame:\n",
    "    @staticmethod\n",
    "    def start_game():\n",
    "        correct = random.randint(0, 10)\n",
    "        print(\"Start  of game, guess a number from 0 to 10\")\n",
    "        for i in range(3):\n",
    "            answer = None\n",
    "            while answer is None:\n",
    "                answer = input(f\"you got {3 - i} tries: \")\n",
    "                try:\n",
    "                    answer = int(answer)\n",
    "                except:\n",
    "                    answer = None\n",
    "            if answer == correct:\n",
    "                print(\"Congrats! You won!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"wrong answer\")\n",
    "        else:\n",
    "            print(\"Game Over\")\n",
    "        play_again = None\n",
    "        while play_again not in (\"y\", \"n\"):\n",
    "            play_again = input(\n",
    "                \"Would you like to play again? ('y', 'n'): \"\n",
    "            )\n",
    "        if play_again == \"y\":\n",
    "            GuessingGame.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b040ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GuessingGame.start_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e47585",
   "metadata": {},
   "source": [
    "Part C  \n",
    "\n",
    "Convert your web scraping code you developed previously into object oriented (use code from flexcar or atp rankings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898021",
   "metadata": {},
   "source": [
    "## Refactor ATP Scraping Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311416e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.FileHandler(\"log.txt\"))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name: str, country: str, points: int):\n",
    "        self.name = name\n",
    "        self.country = country\n",
    "        self.points = points\n",
    "\n",
    "\n",
    "class Week:\n",
    "    def __init__(self, url: str):\n",
    "        self.ranks = range(1, 101)\n",
    "        self._players = None\n",
    "\n",
    "\n",
    "class AtpDb:\n",
    "    def __init__(self, soup: BeautifulSoup, weeks: list[Week] = None):\n",
    "        self._weeks = [] if weeks is None else weeks\n",
    "        self._soup = soup\n",
    "        self.response_list = []\n",
    "        self._headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def add_week(self, week):\n",
    "        self._weeks.append(week)\n",
    "\n",
    "    def add_weeks(self, weeks: list[Week]):\n",
    "        self._weeks.extend(weeks)\n",
    "\n",
    "    @property\n",
    "    def weeks(self):\n",
    "        return self._weeks\n",
    "\n",
    "    @property\n",
    "    def soup(self):\n",
    "        return self._soup\n",
    "\n",
    "    def get_all_urls(self) -> list[str]:\n",
    "        \"\"\"Return a list with all weekly urls from the main soup object\"\"\"\n",
    "        select_tag = self.soup.find(\"select\", id=\"dateWeek-filter\")\n",
    "        date_tags = select_tag.find_all(\"option\")\n",
    "        dates = [\n",
    "            (\n",
    "                date_tag[\"value\"]\n",
    "                if not \"Current\" in date_tag[\"value\"]\n",
    "                else \"Current+Date\"\n",
    "            )\n",
    "            for date_tag in date_tags\n",
    "        ]\n",
    "        base_url = (\n",
    "            \"https://www.atptour.com/en/rankings/singles?dateWeek=\"\n",
    "        )\n",
    "        return sorted([f\"{base_url}{date}\" for date in dates])\n",
    "\n",
    "    def get_responses(self):\n",
    "        urls = self.get_all_urls()\n",
    "        urls_length = len(urls)\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            logger.info(\n",
    "                f\"processing request {i + 1} out of {urls_length}\"\n",
    "            )\n",
    "            time.sleep(random.uniform(0.3, 0.4))\n",
    "            response = requests.get(url, headers=self._headers)\n",
    "            if response.status_code != 200:\n",
    "                response_list = []\n",
    "                raise ConnectionError(\n",
    "                    f\"failed to fetch data from {url}\"\n",
    "                )\n",
    "            self.response_list.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a43e6",
   "metadata": {},
   "source": [
    "#### Original Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_all_urls(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"Return a list with all weekly urls from the main soup object\"\"\"\n",
    "    select_tag = soup.find(\"select\", id=\"dateWeek-filter\")\n",
    "    date_tags = select_tag.find_all(\"option\")\n",
    "    dates = [date_tag[\"value\"] if not \"Current\" in date_tag[\"value\"] else \"Current+Date\" for date_tag in date_tags]\n",
    "    base_url = \"https://www.atptour.com/en/rankings/singles?dateWeek=\"\n",
    "    return sorted([f\"{base_url}{date}\" for date in dates])\n",
    "\n",
    "\n",
    "def get_active_week(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    returns the active week, directly through scraping and not through the url\n",
    "    unlike the get_all_urls it will extract the text contents instead of \n",
    "    the value attribute, otherwise we would get date 'Current-Date' for our\n",
    "    most recent date.\n",
    "    \"\"\"\n",
    "    \n",
    "    select_tag = soup.find(\"select\", id=\"dateWeek-filter\")\n",
    "    return select_tag.find(\"option\", selected=True).text\n",
    "\n",
    "\n",
    "def get_player_names(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    returns a list of the top-100 players names ordered by rank\n",
    "    \"\"\"\n",
    "    li_tags = soup.find_all(\"li\", class_=\"name center\")\n",
    "    return [li.find(\"span\").text for li in li_tags]\n",
    "\n",
    "\n",
    "def get_countries(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    returns a list of the top-100 players countries ordered by rank\n",
    "    \"\"\"\n",
    "    svg_tags = soup.find_all(\"svg\", class_=\"atp-flag\")[:100]\n",
    "    use_tags = [svg_tag.find(\"use\") for svg_tag in svg_tags]\n",
    "    links = [use_tag[\"href\"] for use_tag in use_tags]\n",
    "    countries = [_extract_flag_abbr(link) for link in links]\n",
    "    return _convert_flag_abbr(soup, countries)\n",
    "\n",
    "\n",
    "def _extract_flag_abbr(string: str) -> str:\n",
    "    \"\"\"\n",
    "    extract the flag abbreviation out of a link to the flag png\n",
    "    \"\"\"\n",
    "    return re.search(r\"(?<=#flag-)[A-Za-z]{3}$\", string).group(0)\n",
    "\n",
    "\n",
    "def _convert_flag_abbr(\n",
    "    soup: BeautifulSoup, countries: list[str]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    helper function to convert a flag/country abbreviation to the \n",
    "    actual country name. The relationship between the can be find inside\n",
    "    the source code and I did not have rely on external sources\n",
    "    \"\"\"\n",
    "    select_region_filter = soup.find(\"select\", id=\"region-filter\")\n",
    "    region_option_tags = select_region_filter.find_all(\"option\")\n",
    "    countries_tuple = [\n",
    "        (region_option_tag[\"value\"], region_option_tag.text)\n",
    "        for region_option_tag in region_option_tags\n",
    "    ]\n",
    "    country_dict = {k.lower(): v for k, v in countries_tuple}\n",
    "    return [\n",
    "        country_dict.get(country_abbr, country_abbr)\n",
    "        for country_abbr in countries\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_points(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    after some debugging a edge case failures, I adjusted and tested\n",
    "    the points extraction to the following code.\n",
    "    I essentially had to find the tag before the one I was looking for, \n",
    "    and the seek the sibling.\n",
    "    The reason for picking the slice is that I sometimes get 101 results \n",
    "    instead of 100 and in that case the first one does not lead to any points.\n",
    "    Taking the slice of the last 100 seems safe.\n",
    "    \"\"\"\n",
    "    points_tds = soup.find_all(\"td\", class_=\"age small-cell\")\n",
    "    return [\n",
    "        points_td.find_next_sibling(\"td\").find(\"a\").text.strip()\n",
    "        for points_td in points_tds[-100:]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url = \"https://www.atptour.com/en/rankings/singles\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.status_code\n",
    "# Check status code\n",
    "if response.status_code != 200:\n",
    "    print(response.status_code, response.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.FileHandler(\"log.txt\"))\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all responses\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "urls = get_all_urls(soup)\n",
    "urls_length = len(urls)\n",
    "\n",
    "response_list = []\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    logger.info(f\"processing request {i + 1} out of {urls_length}\")\n",
    "    time.sleep(random.uniform(0.3, 0.4))\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        response_list = []\n",
    "        raise ConnectionError(f'failed to fetch data from {url}')\n",
    "    response_list.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html to files\n",
    "for i, response in enumerate(response_list):\n",
    "    if response and response.status_code == 200:\n",
    "        with open(f\"pages/page_{i}.html\", \"wb\") as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files to a list of html text\n",
    "import glob\n",
    "\n",
    "content_list = []\n",
    "file_paths = sorted(glob.glob(\"pages/page_*.html\"))\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1197abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all elements of the html list to a list of soup objects (parsed)\n",
    "# extract data and load them into a list of dictionaries\n",
    "content_length = len(content_list)\n",
    "results_list = []\n",
    "\n",
    "for i, content in enumerate(content_list):\n",
    "    logger.info(f\"processing response {i + 1} out of {content_length}\")\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    week = get_active_week(soup)\n",
    "    ranks = list(range(1, 101))\n",
    "    player_names = get_player_names(soup)\n",
    "    countries = get_countries(soup)\n",
    "    points_all = get_points(soup)\n",
    "    sizes = map(len, [ranks, player_names, countries, points_all])\n",
    "    # if not len(set(sizes)) == 1 or not all(v > 0 for v in sizes):\n",
    "    if not all(v > 0 for v in sizes):\n",
    "        logger.info(\n",
    "            f\"week {week} error\\nranks: {ranks}\\nplayer_names: {player_names}\\ncountries: {countries} \\n points_all: {points_all}\"\n",
    "        )\n",
    "\n",
    "    results_list.extend(\n",
    "        [\n",
    "            {\n",
    "                \"week\": week,\n",
    "                \"rank\": rank,\n",
    "                \"player_name\": player_name,\n",
    "                \"country\": country,\n",
    "                \"points\": points,\n",
    "            }\n",
    "            for rank, player_name, country, points in zip(\n",
    "                ranks, player_names, countries, points_all\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bb_data_edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
